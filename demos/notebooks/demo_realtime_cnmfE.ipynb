{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for real-time processing of microendoscopic data with CaImAn\n",
    "This demo presents 3 approaches for processing microendoscopic data in real time using CaImAn. \n",
    "1. Sufficiently long initialization phase to identify all ROIs  followed by tracking\n",
    "2. Short initalization phase followed by online processing using OnACID-E \n",
    "3. Short initalization phase followed by online processing using Ring-CNN+OnACID\n",
    "\n",
    "All approached include:\n",
    "- Motion Correction using the NoRMCorre algorithm\n",
    "- Source extraction using a variant of the CNMF algorithm\n",
    "- Deconvolution using the OASIS algorithm\n",
    "\n",
    "OnACID-E and Ring-CNN further include\n",
    "- Detection of new components\n",
    "- Refinement of neural spatial footprints (and background in the case of OnACID-E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "import bokeh.plotting as bpl\n",
    "from copy import deepcopy\n",
    "import cv2\n",
    "import holoviews as hv\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Process, Queue\n",
    "import numpy as np\n",
    "from time import time, sleep, perf_counter\n",
    "\n",
    "logging.basicConfig(format=\n",
    "                          \"%(relativeCreated)12d [%(filename)s:%(funcName)10s():%(lineno)s] [%(process)d] %(message)s\",\n",
    "                    # filename=\"/tmp/caiman.log\",\n",
    "                    level=logging.WARNING)\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.source_extraction import cnmf\n",
    "from caiman.source_extraction.cnmf.online_cnmf import demix1p\n",
    "from caiman.utils.nn_models import (fit_NL_model, create_LN_model, quantile_loss, rate_scheduler)\n",
    "from caiman.utils.utils import download_demo, caiman_datadir\n",
    "from caiman.utils.visualization import nb_inspect_correlation_pnr\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we mention the code that can be used to read directly from the camera.<br/>\n",
    "In this demo we simulate it by reading frame by frame from a file instead, so that everybody can execute the demo.\n",
    "The `download_demo` function will download the file (if not already present) and store it inside your caiman_data/example_movies folder. We create an `iterator` over the file that returns the next imaged frame when calling `next(iterator)`.<br/>\n",
    "Here we define a function that creates and returns such an iterator. It works for both, a file or an imaging device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iterator(device=0, fr=None):\n",
    "    \"\"\"\n",
    "    device: device number (int) or filename (string) for reading from camera or file respectively\n",
    "    fr: frame rate\n",
    "    \"\"\"\n",
    "    if type(device) is int:  # capture from camera\n",
    "        def capture_iter(device=device, fr=fr):\n",
    "            cap = cv2.VideoCapture(device)\n",
    "            if fr is not None:  # set frame rate\n",
    "                cap.set(cv2.CAP_PROP_FPS, fr)\n",
    "            while True:\n",
    "                yield cv2.cvtColor(cap.read()[1], cv2.COLOR_BGR2GRAY)\n",
    "        iterator = capture_iter(device, fr)\n",
    "    else:  # read frame by frame from file\n",
    "        iterator = cm.base.movies.load_iter(device, var_name_hdf5='Y')\n",
    "    return iterator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sufficiently long initialization phase to identify all ROIs  followed by tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record for few minutes\n",
    "The `download_demo` function will download the file (if not already present) and store it inside your caiman_data/example_movies folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_batch = 3000  # number of frames to use for initialization\n",
    "T = 6000           # total number of frames \n",
    "fr = 10            # frame rate (Hz)\n",
    "\n",
    "iterator = get_iterator(download_demo('blood_vessel_10Hz.mat'))\n",
    "\n",
    "m = cm.movie(np.array([next(iterator) for t in range(init_batch)], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a break from imaging to process recorded data\n",
    "Taking a break to keep this demo simple. One could in parallel continue to save the otherwise \"lost\" frames to disk if one was not only intersted in the real-time experiment but post-analysis of the entire session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_init = m.save('init.mmap', order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_dict = {'fnames': fname_init,\n",
    "               'fr': fr,\n",
    "               'method_init': 'corr_pnr',\n",
    "               'K': None,\n",
    "               'gSig': (3, 3),\n",
    "               'gSiz': (13, 13),\n",
    "               'merge_thr': .65,\n",
    "               'p': 1,\n",
    "               'tsub': 1,\n",
    "               'ssub': 1,\n",
    "               'only_init': True,\n",
    "               'nb': 0,\n",
    "               'min_corr': .7,\n",
    "               'min_pnr': 7,\n",
    "               'normalize_init': False,\n",
    "               'ring_size_factor': 1.4,\n",
    "               'center_psf': True,\n",
    "               'ssub_B': 2,\n",
    "               'init_iter': 1,\n",
    "               's_min': -10,\n",
    "               'init_batch': init_batch,\n",
    "               'init_method': 'cnmf',\n",
    "               'use_dense': False,\n",
    "               'motion_correct': True,     # flag for performing motion correction\n",
    "               'gSig_filt': (3, 3)}        # size of high pass spatial filtering, used in 1p data             \n",
    "opts = cnmf.params.CNMFParams(params_dict=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm1 = cnmf.online_cnmf.OnACID(dview=None, params=opts)\n",
    "cnm1.initialize_online(T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively we could initialize using any other pipeline that models the movie data as $Y=A C + B + N$ with spatial components $A$, temporal components $C$, background $B$, and noise $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def init_from_other_pipeline(cnm, movie, A, C, T):\n",
    "#     Yr = movie.reshape(len(m), -1).T\n",
    "#     e = cnm.estimates\n",
    "#     e.A = A\n",
    "#     e.C = np.zeros_like(C)\n",
    "#     e.bl = np.zeros(len(C))\n",
    "#     e.c1 = np.zeros(len(C))\n",
    "#     e.g = np.zeros((len(C), 1))\n",
    "#     e.neurons_sn = np.zeros(len(C))\n",
    "#     e.S = np.zeros_like(C)\n",
    "#     e.lam = np.zeros(len(C))\n",
    "#     for i,c in enumerate(C):\n",
    "#         e.C[i], e.bl[i], e.c1[i], e.g[i], e.neurons_sn[i], e.S[i], e.lam[i] = (\n",
    "#         cm.source_extraction.cnmf.deconvolution.constrained_foopsi(\n",
    "#         c, p=1, bas_nonneg=False, noise_method='mean', fudge_factor=.97, optimize_g=5))\n",
    "#     e.YrA = C-e.C\n",
    "#     e.W, e.b = cm.source_extraction.cnmf.initialization.compute_W(\n",
    "#         Yr, e.A, e.C, movie.shape[1:],\n",
    "#         cnm.params.get('init', 'gSiz')[0] * cnm.params.get('init', 'ring_size_factor'),\n",
    "#         ssub=cnm.params.get('init', 'ssub_B'))\n",
    "#     cnm._prepare_object(Yr, T)\n",
    "#     return cnm\n",
    "#\n",
    "# replace A,C with the results of your pipeline, and m with your initialization movie \n",
    "# A, C = cnm.estimates.Ab, cnm.estimates.noisyC[:cnm.N]\n",
    "# cnm = cnmf.online_cnmf.OnACID(dview=None, params=opts)\n",
    "# cnm = init_from_other_pipeline(cnm, m, A, C, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start real-time processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm1.t_read = []\n",
    "cnm1.t_demix = []\n",
    "cnm1.t_deconvolve = []\n",
    "est = cnm1.estimates\n",
    "ssub_B = cnm1.params.get('init', 'ssub_B') * cnm1.params.get('init', 'ssub')\n",
    "for t in range(init_batch, T):\n",
    "    # read frame\n",
    "    t0 = time()\n",
    "    frame = next(iterator)\n",
    "    cnm1.t_read.append(time()-t0)\n",
    "    # motion correct\n",
    "    t0 = time()\n",
    "    frame = cnm1.mc_next(t, frame.astype(np.float32))\n",
    "    cnm1.t_motion.append(time()-t0)\n",
    "    # get noisy fluorescence value via NNLS (project data on shapes & demix)\n",
    "    t0 = time()\n",
    "    C_in = est.noisyC[:cnm1.M, t - 1].copy()\n",
    "    est.C_on[:cnm1.M, t], est.noisyC[:cnm1.M, t] = demix1p(\n",
    "        frame.ravel(order='F'), est.Ab, C_in, est.AtA,\n",
    "        Atb=est.Atb, AtW=est.AtW, AtWA=est.AtWA,\n",
    "        iters=3, groups=est.groups, ssub_B=ssub_B, \n",
    "        downscale_matrix=est.downscale_matrix if ssub_B > 1 else None)\n",
    "    cnm1.t_demix.append(time()-t0)\n",
    "    # denoise & deconvolve\n",
    "    t0 = time()\n",
    "    if cnm1.params.get('preprocess', 'p'):\n",
    "        for i, o in enumerate(est.OASISinstances):\n",
    "            o.fit_next(est.noisyC[i, t])\n",
    "            est.C_on[i, t - o.get_l_of_last_pool() +\n",
    "                      1: t + 1] = o.get_c_of_last_pool()\n",
    "    cnm1.t_deconvolve.append(time()-t0)\n",
    "    # add code to display whatever you want in order to guide the closed-loop experiment below\n",
    "    # e.g. print the indices of neurons that just spiked\n",
    "#     print('\\r', ' '*200, end=\"\\r\")\n",
    "#     for i, o in enumerate(est.OASISinstances):\n",
    "#         if o.get_l_of_last_pool() == 1:\n",
    "#             print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate time one would have to wait for next frame to arrive if reading directly from camera\n",
    "t_all = np.cumsum(cnm1.t_read) + np.cumsum(cnm1.t_motion) + np.cumsum(cnm1.t_demix) + np.cumsum(cnm1.t_deconvolve)\n",
    "\n",
    "t_wait=[1]\n",
    "t_wait_total=0\n",
    "for i in range(1, T-init_batch):\n",
    "    t_wait.append(max(i/fr - t_all[i-1]-t_wait_total, 0))\n",
    "    t_wait_total += t_wait[-1]\n",
    "realtime = np.array(t_wait)>0\n",
    "print('%g%s processed in real time. %g/%g frames' % \n",
    "      (100 * realtime.sum() / (T-init_batch), '%', realtime.sum(), T-init_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "for i, f in enumerate((lambda a: 1000*np.array(a), np.cumsum)):\n",
    "    plt.subplot(1,2,1+i)\n",
    "    plt.stackplot(np.arange(T-init_batch), f(cnm1.t_read), f(cnm1.t_motion),\n",
    "                  f(cnm1.t_demix), f(cnm1.t_deconvolve))\n",
    "    plt.gca().add_artist(plt.legend(labels=['read', 'motion', 'demix', 'deconvolve'], loc=2))\n",
    "    plt.title('Processing time allocation')\n",
    "    plt.xlabel('Frame #')\n",
    "    plt.ylabel(('Processing time per frame [ms]', 'Cumulative processing time [s]')[i])\n",
    "    if i==0:\n",
    "        plt.fill_between(range(T-init_batch),[0]*(T-init_batch),\n",
    "                 [plt.ylim()[1]]*(T-init_batch), where=realtime, \n",
    "                 color='y', alpha=.1, edgecolor='y', zorder=-11, label='real time')\n",
    "        plt.gca().add_artist(plt.legend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm1.estimates.A = cnm1.estimates.Ab\n",
    "cnm1.estimates.C = cnm1.estimates.C_on\n",
    "cnm1.estimates.YrA = cnm1.estimates.noisyC-cnm1.estimates.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn, pnr = cm.summary_images.correlation_pnr(cm.load(fnames, var_name_hdf5='Y'), gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n",
    "cnm1.estimates.coordinates = None\n",
    "cnm1.estimates.plot_contours_nb(img=cn, thr=.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm1.estimates.nb_view_components(img=cn, denoised_color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Short initalization phase followed by online processing using OnACID-E "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record for some seconds\n",
    "The `download_demo` function will download the file (if not already present) and store it inside your caiman_data/example_movies folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_batch = 200  # number of frames to use for initialization\n",
    "T = 6000          # total number of frames \n",
    "fr = 10           # frame rate (Hz)\n",
    "\n",
    "iterator = get_iterator(download_demo('blood_vessel_10Hz.mat'))\n",
    "\n",
    "m = cm.movie(np.array([next(iterator) for t in range(init_batch)], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a break from imaging to process recorded data\n",
    "Taking a break to keep this demo simple. One could in parallel continue to save the otherwise \"lost\" frames to disk if one was not only intersted in the real-time experiment but post-analysis of the entire session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_init = m.save('init.mmap', order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_dict = {'fnames': fname_init,\n",
    "               'fr': fr,\n",
    "               'method_init': 'corr_pnr',\n",
    "               'K': None,\n",
    "               'gSig': (3, 3),\n",
    "               'gSiz': (13, 13),\n",
    "               'merge_thr': .65,\n",
    "               'p': 1,\n",
    "               'tsub': 1,\n",
    "               'ssub': 1,\n",
    "               'only_init': True,\n",
    "               'nb': 0,\n",
    "               'min_corr': .7,\n",
    "               'min_pnr': 7,\n",
    "               'normalize_init': False,\n",
    "               'ring_size_factor': 1.4,\n",
    "               'center_psf': True,\n",
    "               'ssub_B': 2,\n",
    "               'init_iter': 1,\n",
    "               's_min': -10,\n",
    "               'init_batch': init_batch,\n",
    "               'init_method': 'cnmf',\n",
    "               'batch_update_suff_stat': True,\n",
    "               'update_freq': 200,\n",
    "               'expected_comps': 700, \n",
    "               'update_num_comps': True, \n",
    "               'rval_thr': .3,\n",
    "               'thresh_fitness_raw': -70,\n",
    "               'thresh_fitness_delta': -20,\n",
    "               'min_SNR': 2.5, \n",
    "               'min_num_trial': 3,\n",
    "               'max_num_added': 3,\n",
    "               'use_corr_img': True,\n",
    "               'use_dense': False,\n",
    "               'motion_correct': True,     # flag for performing motion correction\n",
    "               'gSig_filt': (3, 3),        # size of high pass spatial filtering, used in 1p data\n",
    "               'use_cnn': False}               \n",
    "opts = cnmf.params.CNMFParams(params_dict=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm2 = cnmf.online_cnmf.OnACID(dview=None, params=opts)\n",
    "cnm2.initialize_online(T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start real-time processing \n",
    "The spatial footprints and background parameters are updated every 200 frames. This results in a long processing time for the individual frames for which these updates occur. Thus instead of the usual waiting for the camera to provide the next image, there are few frames for which the images are acquired faster than processed. We use a separate `Process` to acquire and add the images to a FIFO `Queue` at regular time intervals. The main process reads the next image from this `Queue` and waits for it if the `Queue` is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "\n",
    "def append_to_queue():\n",
    "    t_append = []\n",
    "    clock = []\n",
    "    for t in range(init_batch, T):\n",
    "        # read frame and append to queue\n",
    "        t0 = time()\n",
    "        clock.append(time())\n",
    "        frame = next(iterator)\n",
    "        q.put(frame)\n",
    "        t_append.append(time()-t0)\n",
    "    q.put((t_append, clock))\n",
    "    \n",
    "producer = Process(target=append_to_queue)\n",
    "producer.start()\n",
    "cnm2.t_read = []\n",
    "cnm2.t_motion = []\n",
    "cnm2.t_fit = []\n",
    "for t in range(init_batch, T):\n",
    "    # read form queue (wait for next frame if empty)\n",
    "    t0=time()\n",
    "    frame = q.get()\n",
    "    cnm2.t_read.append(time()-t0)\n",
    "    # motion correct\n",
    "    t0 = time()\n",
    "    frame = cnm2.mc_next(t, frame)\n",
    "    cnm2.t_motion.append(time()-t0)\n",
    "    # fit\n",
    "    t0 = time()\n",
    "    cnm2.fit_next(t, frame.ravel(order='F'))\n",
    "    cnm2.t_fit.append(time()-t0)\n",
    "    # add code to display whatever you want in order to guide the closed-loop experiment below\n",
    "cnm2.t_append, clock = q.get()\n",
    "producer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate time one would have to wait for next frame to arrive if reading directly from camera\n",
    "t_all = np.cumsum(cnm2.t_read) + np.cumsum(cnm2.t_motion) + np.cumsum(cnm2.t_fit)\n",
    "\n",
    "t_wait=[1]\n",
    "t_wait_total=0\n",
    "for i in range(1, T-init_batch):\n",
    "    t_wait.append(max(i/fr - t_all[i-1]-t_wait_total, 0))\n",
    "    t_wait_total += t_wait[-1]\n",
    "realtime = np.array(t_wait)>0\n",
    "print('%g%s processed in real time. %g/%g frames' % \n",
    "      (100 * realtime.sum() / (T-init_batch), '%', realtime.sum(), T-init_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "for i, f in enumerate((lambda a: 1000*np.array(a), np.cumsum)):\n",
    "    plt.subplot(1,2,1+i)\n",
    "    plt.stackplot(np.arange(len(cnm2.t_fit)), f(cnm2.t_read), f(cnm2.t_motion),\n",
    "                  f(np.array(cnm2.t_fit) - np.array([cnm2.t_detect, cnm2.t_shapes, cnm2.t_stat]).sum(0)),\n",
    "                  f(cnm2.t_detect), f(cnm2.t_shapes)+f(cnm2.t_stat))\n",
    "    plt.gca().add_artist(plt.legend(labels=['read', 'motion', 'process', 'detect', 'shapes'], loc=2))\n",
    "    plt.title('Processing time allocation')\n",
    "    plt.xlabel('Frame #')\n",
    "    plt.ylabel(('Processing time per frame [ms]', 'Cumulative processing time [s]')[i])\n",
    "    if i==0:\n",
    "        plt.ylim(0, 100)\n",
    "        plt.fill_between(range(T-init_batch),[0]*(T-init_batch),\n",
    "                 [100]*(T-init_batch), where=realtime, \n",
    "                 color='y', alpha=.1, edgecolor='y', zorder=-11, label='real time')\n",
    "        plt.gca().add_artist(plt.legend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm2.estimates.A = cnm2.estimates.Ab\n",
    "cnm2.estimates.C = cnm2.estimates.C_on[:cnm2.N]\n",
    "cnm2.estimates.YrA = cnm2.estimates.noisyC[:cnm2.N]-cnm2.estimates.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn, pnr = cm.summary_images.correlation_pnr(cm.load(fnames, var_name_hdf5='Y'), gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n",
    "cnm2.estimates.coordinates = None\n",
    "cnm2.estimates.plot_contours_nb(img=cn, thr=.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm2.estimates.nb_view_components(img=cn, denoised_color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Short initalization phase followed by online processing using Ring-CNN+OnACID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record for some seconds\n",
    "The `download_demo` function will download the file (if not already present) and store it inside your caiman_data/example_movies folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_batch = 500  # number of frames to use for initialization\n",
    "T = 6000          # total number of frames \n",
    "fr = 10           # frame rate (Hz)\n",
    "\n",
    "iterator = get_iterator(download_demo('blood_vessel_10Hz.mat'))\n",
    "\n",
    "m = cm.movie(np.array([next(iterator) for t in range(init_batch)], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a break from imaging to process recorded data\n",
    "Taking a break to keep this demo simple. One could in parallel continue to save the otherwise \"lost\" frames to disk if one was not only intersted in the real-time experiment but post-analysis of the entire session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_init = m.save('init.mmap', order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_model = False                                                 # set to True to re-use an existing ring model\n",
    "path_to_model = None                                                # specify a pre-trained model here if needed \n",
    "gSig = (7, 7)                                                       # expected half size of neurons\n",
    "gnb = 2                                                             # number of background components for OnACID\n",
    "\n",
    "params_dict = {'fnames': fname_init,\n",
    "               'var_name_hdf5': 'Y',                                # name of variable inside mat file where the data is stored\n",
    "               'fr': fr,                                            # frame rate (Hz)\n",
    "               'decay_time': 0.5,                                   # approximate length of transient event in seconds\n",
    "               'gSig': gSig,\n",
    "               'p': 1,                                              # order of AR indicator dynamics\n",
    "               'ring_CNN': True,                                    # SET TO TRUE TO USE RING CNN \n",
    "               'min_SNR': 2.65,                                     # minimum SNR for accepting new components\n",
    "               'SNR_lowest': 0.75,                                  # reject components with SNR below this value\n",
    "               'use_cnn': False,                                    # do not use CNN based test for components\n",
    "               'use_ecc': True,                                     # test eccentricity\n",
    "               'max_ecc': 2.625,                                    # reject components with eccentricity above this value\n",
    "               'rval_thr': 0.70,                                    # correlation threshold for new component inclusion\n",
    "               'rval_lowest': 0.25,                                 # reject components with corr below that value\n",
    "               'ds_factor': 1,                                      # spatial downsampling factor (increases speed but may lose some fine structure)\n",
    "               'nb': gnb,\n",
    "               'motion_correct': True,                              # Flag for motion correction\n",
    "               'init_batch': init_batch,                            # number of frames for initialization (presumably from the first file)\n",
    "               'init_method': 'bare',\n",
    "               'normalize': False,\n",
    "               'expected_comps': 700,                                # maximum number of expected components used for memory pre-allocation (exaggerate here)\n",
    "               'sniper_mode': False,                                 # flag using a CNN to detect new neurons (o/w space correlation is used)\n",
    "               'dist_shape_update' : True,                           # flag for updating shapes in a distributed way\n",
    "               'min_num_trial': 5,                                   # number of candidate components per frame\n",
    "               'epochs': 1,                                          # number of total passes over the data\n",
    "               'stop_detection': False,                              # Run a last epoch without detecting new neurons  \n",
    "               'K': 50,                                              # initial number of components\n",
    "               'lr': 6e-4,\n",
    "               'lr_scheduler': [0.9, 6000, 10000],\n",
    "               'pct': 0.01,\n",
    "               'path_to_model': path_to_model,                       # where the ring CNN model is saved/loaded\n",
    "               'reuse_model': reuse_model,                           # flag for re-using a ring CNN model\n",
    "              }\n",
    "opts = cnmf.params.CNMFParams(params_dict=params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train or load Ring CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm3 = cnmf.online_cnmf.OnACID(params=opts)\n",
    "\n",
    "if cnm3.params.get('ring_CNN', 'loss_fn') == 'pct':\n",
    "    loss_fn = quantile_loss(cnm3.params.get('ring_CNN', 'pct'))\n",
    "else:\n",
    "    loss_fn = cnm3.params.get('ring_CNN', 'loss_fn')\n",
    "if cnm3.params.get('ring_CNN', 'lr_scheduler') is None:\n",
    "    sch = None\n",
    "else:\n",
    "    sch = rate_scheduler(*cnm3.params.get('ring_CNN', 'lr_scheduler'))\n",
    "model_LN = create_LN_model(m, shape=m.shape[1:] + (1,),\n",
    "                           n_channels=cnm3.params.get('ring_CNN', 'n_channels'),\n",
    "                           lr=cnm3.params.get('ring_CNN', 'lr'),\n",
    "                           gSig=cnm3.params.get('init', 'gSig')[0],\n",
    "                           loss=loss_fn, width=cnm3.params.get('ring_CNN', 'width'),\n",
    "                           use_add=cnm3.params.get('ring_CNN', 'use_add'),\n",
    "                           use_bias=cnm3.params.get('ring_CNN', 'use_bias'))\n",
    "if cnm3.params.get('ring_CNN', 'reuse_model'):\n",
    "    model_LN.load_weights(cnm3.params.get('ring_CNN', 'path_to_model'))\n",
    "else:\n",
    "    model_LN, history, path_to_model = fit_NL_model(\n",
    "        model_LN, m, epochs=cnm3.params.get('ring_CNN', 'max_epochs'),\n",
    "        patience=cnm3.params.get('ring_CNN', 'patience'), schedule=sch)\n",
    "    cnm3.params.set('ring_CNN', {'path_to_model': path_to_model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize OnACID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm3.initialize_online(T=T, model_LN=model_LN)\n",
    "\n",
    "# dummy prediction to initialize CNN-ring model model_LN\n",
    "model_LN.predict(m[:1,...,None]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start real-time processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm3.t_read = []\n",
    "cnm3.t_bkgrd = []\n",
    "cnm3.t_motion = []\n",
    "cnm3.t_fit = []\n",
    "for t in range(init_batch, T):\n",
    "    # read frame\n",
    "    t0 = time()\n",
    "    frame = next(iterator)\n",
    "    cnm3.t_read.append(time()-t0)\n",
    "    # remove background\n",
    "    t0 = time()\n",
    "    frame = np.maximum(frame - np.squeeze(model_LN.predict(\n",
    "        frame.astype(np.float32)[None,...,None])), 0)\n",
    "    cnm3.t_bkgrd.append(time()-t0)\n",
    "    # motion correct\n",
    "    t0 = time()\n",
    "    frame = cnm3.mc_next(t, frame.astype(np.float32))\n",
    "    cnm3.t_motion.append(time()-t0)\n",
    "    # fit\n",
    "    t0 = time()\n",
    "    cnm3.fit_next(t, frame.ravel(order='F'))\n",
    "    cnm3.t_fit.append(time()-t0)\n",
    "    # add code to display whatever you want in order to guide the closed-loop experiment below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate time one would have to wait for next frame to arrive if reading directly from camera\n",
    "t_all = np.cumsum(cnm3.t_read) + np.cumsum(cnm3.t_bkgrd) + np.cumsum(cnm3.t_motion) + np.cumsum(cnm3.t_fit)\n",
    "\n",
    "t_wait=[1]\n",
    "t_wait_total=0\n",
    "for i in range(1, T-init_batch):\n",
    "    t_wait.append(max(i/fr - t_all[i-1]-t_wait_total, 0))\n",
    "    t_wait_total += t_wait[-1]\n",
    "realtime = np.array(t_wait)>0\n",
    "print('%g%s processed in real time. %g/%g frames' % \n",
    "      (100 * realtime.sum() / (T-init_batch), '%', realtime.sum(), T-init_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "for i, f in enumerate((lambda a: 1000*np.array(a), np.cumsum)):\n",
    "    plt.subplot(1,2,1+i)\n",
    "    plt.stackplot(np.arange(len(cnm3.t_fit)), f(cnm3.t_read), f(cnm3.t_bkgrd), f(cnm3.t_motion),\n",
    "                  f(np.array(cnm3.t_fit) - np.array([cnm3.t_detect, cnm3.t_shapes, cnm3.t_stat]).sum(0)),\n",
    "                  f(cnm3.t_detect), f(cnm3.t_shapes)+f(cnm3.t_stat))\n",
    "    plt.gca().add_artist(plt.legend(labels=['read', 'background', 'motion', 'process', 'detect', 'shapes'], loc=2))\n",
    "    plt.title('Processing time allocation')\n",
    "    plt.xlabel('Frame #')\n",
    "    plt.ylabel(('Processing time per frame [ms]', 'Cumulative processing time [s]')[i])\n",
    "    if i==0:\n",
    "        plt.fill_between(range(T-init_batch),[0]*(T-init_batch),\n",
    "                 [plt.ylim()[1]]*(T-init_batch), where=realtime, \n",
    "                 color='y', alpha=.1, edgecolor='y', zorder=-11, label='real time')\n",
    "        plt.gca().add_artist(plt.legend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm3.estimates.A = cnm3.estimates.Ab[:,gnb:]\n",
    "cnm3.estimates.C = cnm3.estimates.C_on[gnb:cnm3.M]\n",
    "cnm3.estimates.YrA = cnm3.estimates.noisyC[gnb:cnm3.M]-cnm3.estimates.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn, pnr = cm.summary_images.correlation_pnr(cm.load(fnames, var_name_hdf5='Y'), gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n",
    "cnm3.estimates.coordinates = None\n",
    "cnm3.estimates.plot_contours_nb(img=cn, thr=.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnm3.estimates.nb_view_components(img=cn,denoised_color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:36MKL]",
   "language": "python",
   "name": "conda-env-36MKL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
