{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of online analysis using OnACID\n",
    "\n",
    "Complete pipeline (motion correction + source extraction + deconvolution) for running OnACID.\n",
    "Two-photon mesoscope data kindly provided by the Tolias lab, Baylor college of medicine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "caiman_path = os.path.join(\"..\", \"..\")\n",
    "sys.path.append(caiman_path)\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        print('Debugging!')\n",
    "        # this is used for debugging purposes only. allows to reload classes when changed\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    print('Not IPYTHON')\n",
    "    pass\n",
    "\n",
    "from time import time\n",
    "import caiman as cm\n",
    "from caiman.source_extraction import cnmf as cnmf\n",
    "from caiman.utils.visualization import view_patches_bar\n",
    "from caiman.utils.utils import download_demo, load_object, save_object\n",
    "import pylab as pl\n",
    "import scipy\n",
    "from caiman.motion_correction import motion_correct_iteration_fast\n",
    "import cv2\n",
    "from caiman.utils.visualization import plot_contours, nb_view_patches, nb_plot_contour\n",
    "import glob\n",
    "from caiman.source_extraction.cnmf.online_cnmf import bare_initialization, initialize_movie_online, RingBuffer\n",
    "from copy import deepcopy\n",
    "\n",
    "import bokeh.plotting as bpl\n",
    "try:\n",
    "       from bokeh.io import vform, hplot\n",
    "except:\n",
    "       # newer version of bokeh does not use vform & hplot, instead uses column & row\n",
    "       from bokeh.layouts import column as vform\n",
    "       from bokeh.layouts import row as hplot\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cmap\n",
    "\n",
    "bpl.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First download the data\n",
    "\n",
    "The function ```download_demo``` will look for the datasets ```Tolias_mesoscope_*.hdf5``` inside the folder specified by the variable ```fld_name``` and will download the files if they do not exist. Note that you must be in the main CaImAn folder to run this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld_name = 'Mesoscope'                              # folder inside ./example_movies where files will be saved\n",
    "download_demo('Tolias_mesoscope_1.hdf5',fld_name, caiman_base=caiman_path)\n",
    "download_demo('Tolias_mesoscope_2.hdf5',fld_name, caiman_base=caiman_path)\n",
    "download_demo('Tolias_mesoscope_3.hdf5',fld_name, caiman_base=caiman_path)\n",
    "\n",
    "folder_name = os.path.join(caiman_path, 'example_movies', fld_name) # folder where files are located\n",
    "extension = 'hdf5'                                  # extension of files\n",
    "fls = glob.glob(folder_name + '/*' + extension)       # read all files to be processed \n",
    "\n",
    "print(fls)                                          # your list of files should look something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some parameters\n",
    "\n",
    "Here we set up some parameters for running OnACID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = 15                                                             # frame rate (Hz)\n",
    "decay_time = 0.5                                                    # approximate length of transient event in seconds\n",
    "gSig = (4,4)                                                        # expected half size of neurons\n",
    "p = 1                                                               # order of AR indicator dynamics\n",
    "min_SNR = 2.5                                                       # minimum SNR for accepting new components\n",
    "rval_thr = 0.85                                                     # correlation threshold for new component inclusion\n",
    "ds_factor = 1                                                       # spatial downsampling factor (increases speed but may lose some fine structure)\n",
    "gnb = 2                                                             # number of background components\n",
    "gSig = tuple(np.ceil(np.array(gSig)/ds_factor).astype('int'))       # recompute gSig if downsampling is involved\n",
    "mot_corr = True                                                     # flag for online motion correction \n",
    "max_shift = np.ceil(10./ds_factor).astype('int')                    # maximum allowed shift during motion correction\n",
    "\n",
    "# set up some additional supporting parameters needed for the algorithm (these are default values but change according to dataset characteristics)\n",
    "\n",
    "max_comp_update_shape = np.inf                                      # number of shapes to be updated each time (put this to a finite small value to increase speed)\n",
    "init_files = 1                                                      # number of files used for initialization\n",
    "online_files = len(fls) - 1                                         # number of files used for online\n",
    "initbatch = 200                                                     # number of frames for initialization (presumably from the first file)\n",
    "expected_comps = 300                                                # maximum number of expected components used for memory pre-allocation (exaggerate here)\n",
    "K = 2                                                               # initial number of components\n",
    "N_samples = np.ceil(fr*decay_time)                                  # number of timesteps to consider when testing new neuron candidates\n",
    "thresh_fitness_raw = scipy.special.log_ndtr(-min_SNR)*N_samples     # exceptionality threshold\n",
    "epochs = 2                                                          # number of passes over the data\n",
    "len_file = 1000                                                     # upper bound for number of frames in each file (used right below)\n",
    "T1 = len(fls)*len_file*epochs                                       # total length of all files (if not known use a large number, then truncate at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process the initializing batch of frames\n",
    "\n",
    "The first ```initbatch``` frames are loaded in the memory to serve for initialization purposes. We then motion-correct them to, remove the min value to make the data non-negative, and normalize to equalize the variance amonf the FOV. The correlation image on this small batch is also computed for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if ds_factor > 1:                                   # load only the first initbatch frames and possibly downsample them\n",
    "    Y = cm.load(fls[0], subindices = slice(0,initbatch,None)).astype(np.float32).resize(1. / ds_factor, 1. / ds_factor)\n",
    "else:\n",
    "    Y =  cm.load(fls[0], subindices = slice(0,initbatch,None)).astype(np.float32)\n",
    "    \n",
    "if mot_corr:                                        # perform motion correction on the first initbatch frames\n",
    "    mc = Y.motion_correct(max_shift, max_shift)\n",
    "    Y = mc[0].astype(np.float32)\n",
    "    borders = np.max(mc[1])\n",
    "else:\n",
    "    Y = Y.astype(np.float32)\n",
    "      \n",
    "img_min = Y.min()                                   # minimum value of movie. Subtract it to make the data non-negative\n",
    "Y -= img_min\n",
    "img_norm = np.std(Y, axis=0)                        \n",
    "img_norm += np.median(img_norm)                     # normalizing factor to equalize the FOV\n",
    "Y = Y / img_norm[None, :, :]                        # normalize data\n",
    "\n",
    "_, d1, d2 = Y.shape\n",
    "dims = (d1, d2)                                     # dimensions of FOV\n",
    "Yr = Y.to_2D().T                                    # convert data into 2D array                                    \n",
    "\n",
    "Cn_init = Y.local_correlations(swap_dim = False)    # compute correlation image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "We use the ```bare_initialization``` method which essentially circumvents the CNMF approach and quickly initializes a very small number of strong components (in this case just 2), as well as the spatial and temporal background components. This step also serves as a chance to pass some parameters into the object that will be used later to run OnACID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cnm_init = bare_initialization(Y[:initbatch].transpose(1, 2, 0), init_batch=initbatch, k=K, gnb=gnb,\n",
    "                                 gSig=gSig, p=p, minibatch_shape=100, minibatch_suff_stat=5,\n",
    "                                 update_num_comps = True, rval_thr=rval_thr,\n",
    "                                 thresh_fitness_raw = thresh_fitness_raw,\n",
    "                                 batch_update_suff_stat=True, max_comp_update_shape = max_comp_update_shape, \n",
    "                                 deconv_flag = False, use_dense = True,\n",
    "                                 simultaneously=False, n_refit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results of the initializer\n",
    "\n",
    "Some basic plotting here. Notice that the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crd = nb_plot_contour(Cn_init, cnm_init.A.todense(), dims[0], dims[1])\n",
    "bpl.show(crd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_view_patches(Yr,cnm_init.A.tocsc(),cnm_init.C,cnm_init.b,cnm_init.f,dims[0],dims[1],thr = 0.8,image_neurons=Cn_init, denoised_color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now prepare object to run OnACID\n",
    "\n",
    "To test the algorithm with multiple parameters you may want to save the initialization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "cnm_init._prepare_object(np.asarray(Yr), T1, expected_comps, idx_components=None,\n",
    "                        min_num_trial = 2, N_samples_exceptionality = int(N_samples))\n",
    "\n",
    "save_init = False     # flag for saving initialization object. Useful if you want to check OnACID with different parameters but same initialization\n",
    "if save_init:   \n",
    "    cnm_init.dview = None\n",
    "    save_object(cnm_init, fls[0][:-4] + '_DS_' + str(ds_factor) + '.pkl')\n",
    "    cnm_init = load_object(fls[0][:-4] + '_DS_' + str(ds_factor) + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run OnACID\n",
    "\n",
    "OnACID will start from the frame ```initbatch + 1``` and would go over all frames for all files, first correcting for motion, then demixing the sources and deconvolving their activity, and finally looking for new components over a rolling window. The possibility of more than one pass over the data is also present by appropriately setting the variable ```epochs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "cnm2 = deepcopy(cnm_init)\n",
    "cnm2.Ab_epoch = []                       # save the shapes at the end of each epoch\n",
    "t = cnm2.initbatch\n",
    "tottime = []\n",
    "update_Cn = False                        # flag for updating the max-correlation image\n",
    "Cn = Cn_init.copy()\n",
    "\n",
    "\n",
    "if online_files == 0:                    # check whether there are any additional files\n",
    "    process_files = fls[init_files]      # end processing at this file\n",
    "    init_batc_iter = [initbatch]         # place where to start\n",
    "    end_batch = T1              \n",
    "else:\n",
    "    process_files = fls[:init_files + online_files]     # additional files\n",
    "    init_batc_iter = [initbatch] + [0]*online_files     # where to start reading at each file\n",
    "\n",
    "shifts = []\n",
    "\n",
    "for iter in range(epochs):\n",
    "    if iter > 0:\n",
    "        process_files = fls[:init_files + online_files]     # if not on first epoch process all files from scratch\n",
    "        init_batc_iter = [0]*(online_files+init_files)      #\n",
    "        \n",
    "    for file_count, ffll in enumerate(process_files):  # np.array(fls)[np.array([1,2,3,4,5,-5,-4,-3,-2,-1])]:\n",
    "        print('Now processing file ' + ffll)\n",
    "        Y_ = cm.load(ffll, subindices=slice(init_batc_iter[file_count],T1,None))\n",
    "        \n",
    "        if update_Cn:   # update max-correlation (and perform offline motion correction) just for illustration purposes\n",
    "            if ds_factor > 1:\n",
    "                Y_1 = Y_.resize(1. / ds_factor, 1. / ds_factor, 1)\n",
    "            else:\n",
    "                Y_1 = Y_.copy()                    \n",
    "                if mot_corr:\n",
    "                    templ = (cnm2.Ab.data[:cnm2.Ab.indptr[1]] * cnm2.C_on[0, t - 1]).reshape(cnm2.dims, order='F') * img_norm        \n",
    "                    newcn = (Y_1 - img_min).motion_correct(max_shift, max_shift, template=templ)[0].local_correlations(swap_dim=False)                \n",
    "                    Cn = np.maximum(Cn, newcn)\n",
    "                else:\n",
    "                    Cn = np.maximum(Cn, Y_1.local_correlations(swap_dim=False))\n",
    "    \n",
    "        old_comps = cnm2.N                              # number of existing components\n",
    "        for frame_count, frame in enumerate(Y_):        # now process each file\n",
    "            if np.isnan(np.sum(frame)):\n",
    "                raise Exception('Frame ' + str(frame_count) + ' contains nan')\n",
    "            if t % 200 == 0:\n",
    "                print('Epoch: ' + str(iter+1) + '. ' + str(t)+' frames have beeen processed in total. '+str(cnm2.N - old_comps)+' new components were added. Total number of components is '+str(cnm2.Ab.shape[-1]-gnb))\n",
    "                old_comps = cnm2.N\n",
    "    \n",
    "            t1 = time()                                 # count time only for the processing part\n",
    "            frame_ = frame.copy().astype(np.float32)    # \n",
    "            if ds_factor > 1:\n",
    "                frame_ = cv2.resize(frame_, img_norm.shape[::-1])   # downsample if necessary \n",
    "    \n",
    "            frame_ -= img_min                                       # make data non-negative\n",
    "    \n",
    "            if mot_corr:                                            # motion correct\n",
    "                templ = cnm2.Ab.dot(cnm2.C_on[:cnm2.M, t - 1]).reshape(cnm2.dims, order='F') * img_norm\n",
    "                frame_cor, shift = motion_correct_iteration_fast(frame_, templ, max_shift, max_shift)\n",
    "                shifts.append(shift)\n",
    "            else:\n",
    "                templ = None\n",
    "                frame_cor = frame_\n",
    "    \n",
    "            frame_cor = frame_cor / img_norm                        # normalize data-frame\n",
    "            cnm2.fit_next(t, frame_cor.reshape(-1, order='F'))      # run OnACID on this frame\n",
    "            tottime.append(time() - t1)                             # store time\n",
    "            t += 1\n",
    "            \n",
    "    cnm2.Ab_epoch.append(cnm2.Ab.copy())                        # save the shapes at the end of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save results and do some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing speed was ' + str((t - initbatch) / np.sum(tottime))[:5] + ' frames per second.')\n",
    "save_results = False\n",
    "\n",
    "if save_results:\n",
    "    np.savez('results_analysis_online_MOT_CORR.npz',\n",
    "             Cn=Cn, Ab=cnm2.Ab, Cf=cnm2.C_on, b=cnm2.b, f=cnm2.f,\n",
    "             dims=cnm2.dims, tottime=tottime, noisyC=cnm2.noisyC, shifts=shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% extract results from the objects and do some plotting\n",
    "A, b = cnm2.Ab[:, cnm2.gnb:], cnm2.Ab[:, :cnm2.gnb].toarray()\n",
    "C, f = cnm2.C_on[cnm2.gnb:cnm2.M, t-t//epochs:t], cnm2.C_on[:cnm2.gnb, t-t//epochs:t]\n",
    "noisyC = cnm2.noisyC[:,t-t//epochs:t]\n",
    "b_trace = [osi.b for osi in cnm2.OASISinstances]\n",
    "\n",
    "pl.figure()\n",
    "crd = cm.utils.visualization.nb_plot_contour(Cn,A.todense(),dims[0],dims[1],face_color='purple', line_color='black')\n",
    "bpl.show(crd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View components\n",
    "\n",
    "Now inspect the components extracted by OnACID. Note that if single pass was used then several components would be non-zero only for the part of the time interval indicating that they were detected online by OnACID.\n",
    "\n",
    "Note that if you get data rate error you can start Jupyter notebooks using:\n",
    "'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_view_patches(Yr, A.tocsc(), C, b, f, dims[0], dims[1], YrA = noisyC[cnm2.gnb:cnm2.M] - C, thr = 0.8, image_neurons=Cn, denoised_color='red');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
